{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89177f67-876f-4a13-bfb6-cd677d4fa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0) Install & GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf8a8c-e5b1-47a4-bbfc-a8501a363f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install albumentations==1.4.7 pycocotools opencv-python tqdm\n",
    "\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "!nvidia-smi -L || echo \"No GPU detected (OK, will use CPU—lower IMG_SIZE/BATCH).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1797c1-7324-4d5d-a0d9-59eeb3bf41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Config (your paths + knobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c528a-7056-4498-bf8d-7566722f52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = \"/mmfs1/home/jacks.local/kkumari/FHB_Project/Wheatproject-v1-coco\"\n",
    "OUT_DIR   = \"/mmfs1/home/jacks.local/kkumari/FHB_Project/Outcome/runs_attention_unet\"\n",
    "\n",
    "# Training knobs (lower if on CPU or small GPU)\n",
    "IMG_SIZE  = 768\n",
    "BATCH     = 2\n",
    "VAL_BATCH = 2\n",
    "EPOCHS    = 200\n",
    "LR        = 3e-4\n",
    "BASE_CH   = 64  # 64 = stronger but heavier\n",
    "\n",
    "import os\n",
    "assert os.path.exists(DATA_ROOT), f\"DATA_ROOT not found: {DATA_ROOT}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"DATA_ROOT OK →\", DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05818bff-a66a-41dd-81d2-de554e8b106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Fix COCO JSON (annotations→images, drop missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4646205-f2e9-4c25-825b-5d5db0ffd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    ann_dir   = split_dir / \"annotations\"\n",
    "    img_dir   = split_dir / \"images\"\n",
    "    src_json  = ann_dir / \"_annotations.coco.json\"\n",
    "    if not src_json.exists():\n",
    "        print(f\"Skip {split_dir.name}: no _annotations.coco.json\")\n",
    "        return\n",
    "    dst_json  = ann_dir / \"_annotations.coco.fixed.json\"\n",
    "    coco = json.load(open(src_json, \"r\"))\n",
    "\n",
    "    kept_images, kept_ids = [], set()\n",
    "    fixed, dropped = 0, 0\n",
    "\n",
    "    for im in coco[\"images\"]:\n",
    "        fname = im.get(\"file_name\",\"\")\n",
    "        base  = Path(fname).name\n",
    "        candidates = [\n",
    "            split_dir/fname,\n",
    "            img_dir/base,\n",
    "            split_dir/(\"images/\"+base),\n",
    "            split_dir/(fname.replace(\"annotations/\",\"images/\"))\n",
    "        ]\n",
    "        found = None\n",
    "        for c in candidates:\n",
    "            if c.exists():\n",
    "                found = c; break\n",
    "        if found is None:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        rel = found.relative_to(split_dir).as_posix()\n",
    "        if rel != fname: fixed += 1\n",
    "        im[\"file_name\"] = rel\n",
    "        kept_images.append(im)\n",
    "        kept_ids.add(im[\"id\"])\n",
    "\n",
    "    coco[\"images\"] = kept_images\n",
    "    coco[\"annotations\"] = [a for a in coco[\"annotations\"] if a[\"image_id\"] in kept_ids]\n",
    "\n",
    "    bak = ann_dir / \"_annotations.coco.orig.json\"\n",
    "    if not bak.exists():\n",
    "        shutil.copy(src_json, bak)\n",
    "    json.dump(coco, open(dst_json, \"w\"), indent=2)\n",
    "    print(f\"[{split_dir.name}] fixed:{fixed}  dropped:{dropped}  kept:{len(kept_images)}  anns:{len(coco['annotations'])}\")\n",
    "    print(f\"→ wrote {dst_json}\")\n",
    "\n",
    "for sub in [\"train\",\"valid\",\"test\"]:\n",
    "    if os.path.exists(f\"{DATA_ROOT}/{sub}/annotations/_annotations.coco.json\"):\n",
    "        fix_split(f\"{DATA_ROOT}/{sub}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4efebd-d194-47ae-8a0f-efba48660e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Dataset (COCO → semantic masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cd5a4-3458-47e9-b81c-e2a17a6a67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, cv2, numpy as np, random, torch\n",
    "from pathlib import Path\n",
    "from albumentations import (Compose, HorizontalFlip, VerticalFlip, RandomBrightnessContrast,\n",
    "                            CLAHE, Normalize, PadIfNeeded, LongestMaxSize)\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Label map (robust to old \"Diseased\")\n",
    "NAME2CLS = {\"healthy\":1,\"Healthy\":1,\"unhealthy\":2,\"Unhealthy\":2,\"diseased\":2,\"Diseased\":2}\n",
    "N_CLASSES = 3  # 0 bg, 1 healthy, 2 unhealthy\n",
    "\n",
    "def colorize_mask(mask):\n",
    "    pal = np.array([[0,0,0],[0,255,0],[255,0,0]], dtype=np.uint8)\n",
    "    return pal[mask]\n",
    "\n",
    "class CocoSegDataset(Dataset):\n",
    "    def __init__(self, split_dir, img_size=768, augment=True):\n",
    "        self.split_dir = Path(split_dir)\n",
    "        fixed = self.split_dir/\"annotations/_annotations.coco.fixed.json\"\n",
    "        orig  = self.split_dir/\"annotations/_annotations.coco.json\"\n",
    "        ann_path = fixed if fixed.exists() else orig\n",
    "        assert ann_path.exists(), f\"Missing COCO json for {split_dir}\"\n",
    "        self.coco = COCO(str(ann_path))\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.cat_to_cls = {}\n",
    "        for cat in self.coco.loadCats(self.coco.getCatIds()):\n",
    "            self.cat_to_cls[cat[\"id\"]] = NAME2CLS.get(cat[\"name\"], None)\n",
    "\n",
    "        if augment:\n",
    "            self.tf = Compose([\n",
    "                LongestMaxSize(max_size=img_size),\n",
    "                PadIfNeeded(img_size, img_size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "                HorizontalFlip(p=0.5), VerticalFlip(p=0.2),\n",
    "                RandomBrightnessContrast(p=0.4), CLAHE(p=0.2),\n",
    "                Normalize(), ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.tf = Compose([\n",
    "                LongestMaxSize(max_size=img_size),\n",
    "                PadIfNeeded(img_size, img_size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "                Normalize(), ToTensorV2()\n",
    "            ])\n",
    "\n",
    "    def __len__(self): return len(self.img_ids)\n",
    "\n",
    "    def _build_mask(self, img_info, anns):\n",
    "        h, w = img_info[\"height\"], img_info[\"width\"]\n",
    "        mask = np.zeros((h,w), np.uint8)\n",
    "        # healthy first, then unhealthy overwrites on overlaps\n",
    "        for priority in [1,2]:\n",
    "            for ann in anns:\n",
    "                cls = self.cat_to_cls.get(ann[\"category_id\"], None)\n",
    "                if cls is None or cls != priority: continue\n",
    "                m = self.coco.annToMask(ann).astype(np.uint8)\n",
    "                mask[m==1] = cls\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        info   = self.coco.loadImgs([img_id])[0]\n",
    "        path   = str(self.split_dir / info[\"file_name\"])\n",
    "\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
    "        anns    = self.coco.loadAnns(ann_ids)\n",
    "        mask    = self._build_mask(info, anns)\n",
    "\n",
    "        out = self.tf(image=img, mask=mask)\n",
    "        return out[\"image\"], out[\"mask\"].long(), path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247665e8-511c-48d4-92ad-9ce0d598a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea406e3f-8848-43db-9d8e-7cfe5195f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Attention U-Net Architecture (no in-place ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ee99a-f682-4ebf-9219-ec6df2055383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=False),\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, g_ch, x_ch, inter_ch):\n",
    "        super().__init__()\n",
    "        self.Wg  = nn.Sequential(nn.Conv2d(g_ch, inter_ch, 1, bias=False), nn.BatchNorm2d(inter_ch))\n",
    "        self.Wx  = nn.Sequential(nn.Conv2d(x_ch, inter_ch, 1, bias=False), nn.BatchNorm2d(inter_ch))\n",
    "        self.psi = nn.Sequential(nn.Conv2d(inter_ch, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid())\n",
    "        self.relu= nn.ReLU(inplace=False)\n",
    "    def forward(self, g, x):\n",
    "        g_up = F.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        a = self.relu(self.Wg(g_up) + self.Wx(x))\n",
    "        a = self.psi(a)\n",
    "        return x * a\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up  = nn.ConvTranspose2d(in_ch, in_ch//2, 2, stride=2)\n",
    "        self.att = AttentionBlock(in_ch//2, skip_ch, skip_ch//2)\n",
    "        self.conv = ConvBlock(in_ch//2 + skip_ch, out_ch)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        skip = self.att(x, skip)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, n_classes=3, base=32):\n",
    "        super().__init__()\n",
    "        self.c1 = ConvBlock(in_ch, base)       ; self.p1 = nn.MaxPool2d(2)\n",
    "        self.c2 = ConvBlock(base, base*2)      ; self.p2 = nn.MaxPool2d(2)\n",
    "        self.c3 = ConvBlock(base*2, base*4)    ; self.p3 = nn.MaxPool2d(2)\n",
    "        self.c4 = ConvBlock(base*4, base*8)    ; self.p4 = nn.MaxPool2d(2)\n",
    "        self.c5 = ConvBlock(base*8, base*16)\n",
    "\n",
    "        self.u6 = UpBlock(base*16, base*8,  base*8)\n",
    "        self.u7 = UpBlock(base*8,  base*4,  base*4)\n",
    "        self.u8 = UpBlock(base*4,  base*2,  base*2)\n",
    "        self.u9 = UpBlock(base*2,  base,    base)\n",
    "        self.outc = nn.Conv2d(base, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.c1(x); p1 = self.p1(c1)\n",
    "        c2 = self.c2(p1); p2 = self.p2(c2)\n",
    "        c3 = self.c3(p2); p3 = self.p3(c3)\n",
    "        c4 = self.c4(p3); p4 = self.p4(c4)\n",
    "        c5 = self.c5(p4)\n",
    "\n",
    "        x  = self.u6(c5, c4)\n",
    "        x  = self.u7(x,  c3)\n",
    "        x  = self.u8(x,  c2)\n",
    "        x  = self.u9(x,  c1)\n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00d181-d2b4-43c3-9d47-33ea8f44066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Losses, metrics, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f196041-77e9-4eb5-9f53-eda6e8e707d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, numpy as np\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0, ignore_index=0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "    def forward(self, logits, targets):\n",
    "        C = logits.shape[1]\n",
    "        probs = torch.softmax(logits, dim=1)                 # no in-place ops after this\n",
    "        one_hot = torch.nn.functional.one_hot(targets, C).permute(0,3,1,2).float()\n",
    "        if self.ignore_index is not None:\n",
    "            keep = (targets != self.ignore_index).unsqueeze(1).float()\n",
    "            probs = probs * keep\n",
    "            one_hot = one_hot * keep\n",
    "        dims = (0,2,3)\n",
    "        inter = (probs * one_hot).sum(dims)\n",
    "        denom = probs.sum(dims) + one_hot.sum(dims)\n",
    "        dice = (2*inter + self.smooth) / (denom + self.smooth)\n",
    "        return 1 - dice[1:].mean()  # drop background\n",
    "\n",
    "def per_class_iou(pred, target, num_classes=3):\n",
    "    ious=[]\n",
    "    for cls in range(num_classes):\n",
    "        tp = ((pred==cls)&(target==cls)).sum().item()\n",
    "        fp = ((pred==cls)&(target!=cls)).sum().item()\n",
    "        fn = ((pred!=cls)&(target==cls)).sum().item()\n",
    "        ious.append(float('nan') if (tp+fp+fn)==0 else tp/(tp+fp+fn))\n",
    "    return ious\n",
    "\n",
    "def overlay_rgb(img_t, mask_pred):\n",
    "    import cv2\n",
    "    img = img_t.permute(1,2,0).cpu().numpy()\n",
    "    img = (img*255).clip(0,255).astype(np.uint8)\n",
    "    cm  = colorize_mask(mask_pred.cpu().numpy())\n",
    "    return (0.6*img + 0.4*cm).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6f1ec-798d-4b20-9344-03df90e96697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3df4ae-5af3-4ca7-af33-0c557f67e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "train_dir = f\"{DATA_ROOT}/train\"\n",
    "val_dir   = f\"{DATA_ROOT}/valid\"\n",
    "\n",
    "train_ds = CocoSegDataset(train_dir, img_size=IMG_SIZE, augment=True)\n",
    "val_ds   = CocoSegDataset(val_dir,   img_size=IMG_SIZE, augment=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Start with single-process loading to avoid worker/pickle surprises.\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY  = bool(torch.cuda.is_available())\n",
    "\n",
    "def safe_collate(batch):\n",
    "    \"\"\"\n",
    "    A basic collate function that filters out None values.\n",
    "    \"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                          collate_fn=safe_collate, drop_last=False)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=VAL_BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                          collate_fn=safe_collate, drop_last=False)\n",
    "\n",
    "# quick sanity check (will raise clearly if something’s off)\n",
    "sample = next(iter(train_loader))\n",
    "if sample is None:\n",
    "    raise RuntimeError(\"All samples in first batch were invalid. Check dataset.\")\n",
    "x, y, p = sample\n",
    "print(\"train batch:\", x.shape, y.shape, \"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f944ed-7cc9-4e2c-999e-f14325c0e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7) Train (corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc01fb2-073c-4a0a-8320-633aba25cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2, os\n",
    "\n",
    "# (Optional) help catch anything else\n",
    "import torch as _torch\n",
    "_torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "model  = AttentionUNet(in_ch=3, n_classes=N_CLASSES, base=BASE_CH).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "ce_loss   = torch.nn.CrossEntropyLoss()\n",
    "dice      = DiceLoss(ignore_index=0)\n",
    "\n",
    "best = 0.0\n",
    "save_path = Path(OUT_DIR) / \"best_attention_unet.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    tl=[]\n",
    "    for imgs, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", ncols=80):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = ce_loss(logits, masks) + dice(logits, masks)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tl.append(loss.item())\n",
    "\n",
    "    # validate\n",
    "    model.eval(); vloss=[]; vdice=[]; viou=[]\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, _ in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            logits = model(imgs)\n",
    "            vloss.append( (ce_loss(logits, masks) + dice(logits, masks)).item() )\n",
    "            vdice.append( 1 - dice(logits, masks).item() )\n",
    "            viou.append( per_class_iou(logits.argmax(1).cpu(), masks.cpu(), N_CLASSES) )\n",
    "\n",
    "    mean_vloss = float(np.mean(vloss))\n",
    "    mean_vdice = float(np.mean(vdice))\n",
    "    mean_iou   = np.nanmean(np.array(viou), axis=0)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train {np.mean(tl):.4f} | val {mean_vloss:.4f} | \"\n",
    "          f\"Dice {mean_vdice:.4f} | IoU(bg,healthy,unhealthy) {np.round(mean_iou,4)}\")\n",
    "\n",
    "    if mean_vdice > best:\n",
    "        best = mean_vdice\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch,\n",
    "                    \"img_size\": IMG_SIZE, \"base\": BASE_CH}, save_path)\n",
    "        print(\"  ✅ Saved best:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96f69e-ea47-461a-8e50-454a0ee53d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8) Validation previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5a4e2-0860-41f5-bb95-a395d4cd9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "preview_dir = Path(OUT_DIR)/\"previews\"; preview_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "state = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(state[\"model\"]); model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b,(imgs,masks,paths) in enumerate(val_loader):\n",
    "        pred = model(imgs.to(device)).softmax(1).argmax(1).cpu()\n",
    "        for i in range(imgs.size(0)):\n",
    "            ov = overlay_rgb(imgs[i].cpu(), pred[i])\n",
    "            outp = preview_dir/f\"val_{b:03d}_{i:02d}_{Path(paths[i]).stem}.png\"\n",
    "            cv2.imwrite(str(outp), cv2.cvtColor(ov, cv2.COLOR_RGB2BGR))\n",
    "        if b>1: break\n",
    "\n",
    "print(\"Previews saved to:\", preview_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bce74-17d4-492f-b0ac-f98309e99367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9) Single-image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1cd60-0bec-4557-8f28-103327cb4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, torch\n",
    "from albumentations import Compose, LongestMaxSize, PadIfNeeded, Normalize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "RAW_IMG = f\"{DATA_ROOT}/valid/images/\" + os.listdir(f\"{DATA_ROOT}/valid/images\")[0]\n",
    "\n",
    "tf = Compose([\n",
    "    LongestMaxSize(IMG_SIZE),\n",
    "    PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "    Normalize(), ToTensorV2()\n",
    "])\n",
    "\n",
    "rgb = cv2.cvtColor(cv2.imread(RAW_IMG), cv2.COLOR_BGR2RGB)\n",
    "x = tf(image=rgb)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "state = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(state[\"model\"]); model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x).softmax(1).argmax(1)[0].cpu()\n",
    "\n",
    "ov = overlay_rgb(x[0].cpu(), pred)\n",
    "out_path = f\"{OUT_DIR}/inference_overlay.png\"\n",
    "cv2.imwrite(out_path, cv2.cvtColor(ov, cv2.COLOR_RGB2BGR))\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725e30d-04af-4c68-b583-350dbd15d6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kayode]",
   "language": "python",
   "name": "conda-env-kayode-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
